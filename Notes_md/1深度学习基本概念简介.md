# 深度学习基本概念简介

###### **（2021/10/24）**

### 机器学习概念

Machine Learning ≈ Looking for Function：

- 语音识别：通过函数将语音识别为文字：

​       $f(\texttt{audio})=\texttt{"text"}$

- 图像识别：通过函数识别图像：
  
  $f(\texttt{cat img})=\texttt{Cat}$
  
- 位置识别：识别棋盘位置给出下一步的最佳落点：
  
  $f(\texttt{map})=\texttt{next move}$

### 机器学习任务分类

- Regression：函数的输出是一个标量

  eg：预测明天PM2.5的数值

- Classification：在给出的一些分类里，选出最佳的一个类别

  eg：判断是否属于垃圾邮件

- Structured Learning：创造出有有结构性的东西

  eg：生成图片或文档

### 机器学习过程：

**任务：**根据2020年李宏毅YouTube频道每天的播放量，在已知前一天播放量的情况下预测下一天该频道的播放量。

1. 找到一个带有未知参数的函数
2. 从训练集中定义出损失函数$Loss$
3. 最优化问题（$Optimization$）

#### 1.找到一个带有未知参数的函数

​	函数为$$y=f(\texttt{"某一天的播放量"})$$，假该模型为$$y=b+wx_{1}$$（$$y$$：下一天的播放量，$$x_{1}$$：前一天的播放量，$w$和$b$是位置参数要根据	数据集进行调整）

​	$$y=b+wx_{1}$$称为$Model$，其中$w$称为权重($weight$)，$b$称为偏置值($bias$)，Model中的未知数$w$和$b$统称为$hyperparameters$。

#### 2.从训练集中定义出损失函数$Loss$

​	$Loss$:how good a set of values is.

​	假设$b=0.5k,w=1$，Model即为$y=0.5k+1x_{1}$。

​	在已知2017/01/01观看次数为$4.8k$（$x_{1}=4.8k$）的情况下预测2017/01/02的观看次数，通过$y=0.5k+1x_{1}$的Model进行计算所得	出的结果为$y=5.3k$。

​	而2017/01/02的真实观看次数（真实值被称为label）为$\widehat{y}=4.9k$，计算预测结果和真实结果之间的误差 $e_{1}=|y-\widehat{y}|=0.4k$。

​	 在实际的预测时不止会预测2017/01/02一天的观看次数，而是会预测训练集中每一天的值并计算它与真实值之间的误差，误差组成一	个集合$\{e_{1},e_{2},\cdots,e_{N}\}$。所有误差的平均值被定义为Loss：$L=\frac{1}{N}\sum\limits_{n}e_{n}$。

​		**Loss分类：**

​			1.当误差$e$取预测值$y$和真实值$\widehat{y}$差值的绝对值时，Loss被称为mean absolute error（MAE）

​			2.当误差$e$取预测值$y$和真实值$\widehat{y}$差值的平方时，Loss被称为mean square error（MSE）。

​			3.如果$y$和$\widehat{y}$均为概率值，Loss函数可以选择Cross-entropy。

​	枚举所有Model中未知参数的取值绘制出的Loss等高线图被称为**Error Surface**

![image-20211025235012141](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211025235012141.png)



### 最优化问题（$Optimization$）

​	$Optimization$的目的是寻找到使$Loss$值最小的$Model$中的$hyperparameters$（在本例中为$w$和$b$）：$w^{*},b^{*}=arg\min\limits_{{\color{b} w},b}Loss$

​	**Gradient Descent是一种常用的最优化方法。**

​	先假设Model中仅含有未知参数$w$对最优化过程进行学习，最优化过程可表示为$w^{*}=arg\min\limits_{w}L$。	

​		1. 对Model中的未知参数（$w$）进行随机初始化，初始化结果为$w^{0}$。

​		2. 计算$w=w^{0}$时$w$对Loss函数的微分：$\frac{\partial L}{\partial w}|{w=w^{0}}$，若结果为负则将$w$的值在$w^{0}$的基础上增大，若结果为正则将$w$的值在$w^{0}$的基			础上减小。$w$值变化的大小由微分值$\frac{\partial L}{\partial w}|{w=w^{0}}$和${\color{red} \eta}$的乘积决定：${\color{red} \eta}\frac{\partial L}{\partial w}|{w=w^{0}}$。learning rate：${\color{red} \eta}$ 值需要进行额外的设定。参数			$w$值从$w^{0}$更新到$w^{1}$的变化过程可以表示为$w^{1}\gets w^{0}-{\color{red} \eta}\frac{\partial L}{\partial w}|{w=w^{0}}$。

​		3. 反复update参数$w$的值，达到一定的epoch或微分值为$0$时停止。

​	Gradient Descent可能会陷入local minima而无法正确寻找到global minima。（是假问题但是还未说明原因）

​	和上面的步骤一样，将Gradient Descent推广到$w$和$b$两个参数的情况：

​		1. 对参数$w,b$随机初始化得到$w^{0},b^{0}$。

​		2. 计算微分（实际上不需要手动计算微分值）并更新参数$w,b$：

​			$$\frac{\partial L}{\partial w}|{w=w^{0},b=b^{0}},\quad w^{1}\gets w^{0}-{\color{red} \eta}\frac{\partial L}{\partial w}|{w=w^{0},b=b^{0}}$$

​			$$\frac{\partial L}{\partial b}|{w=w^{0},b=b^{0}},\quad b^{1}\gets b^{0}-{\color{red} \eta}\frac{\partial L}{\partial b}|{w=w^{0},b=b^{0}}$$

​	3. 反复update参数 w,b 的值，达到一定的epoch或微分值为 0 时停止。



### 模型优化初步

在上述步骤的优化后Model寻找到的最佳参数$w^{*}=0.97,b^{*}=0.1k$，此时Model为$y=0.1k+0.97x_{1}$。在2017\sim2020年观看次数的训练数据下$L(w^{*},b^{*})=0.48k$，在Model未知的2021年观看次数数据下$L'=0.58k$。将预测值和真实值进行图形化处理如下图。

![image-20211026084943744](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026084943744.png)

观察图标可知大体是以7天为一个周期，所以我们可以把$feature$设置为7。改进后的模型为$y=b+\sum\limits_{j=1}^{7}w_{j}x_{j}$，该模型预测是以前七天的数据作为$feature$k进行预测下一天的播放量。经过训练后在$2017~2020$上的$Loss=0.38k$，在$2021$上的$Loss=0.49k$，训练的最佳参数如下。

| **b** | **$w_1^∗$** | $w_2^∗ $ | **$w_3^∗$** | **$w_4^∗$** | **$w_5^∗$** | **$w_6^∗$** | **$w_7^∗$** |
| ----- | ----------- | -------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| 0.05k | 0.79        | -0.31    | 0.12        | -0.01       | -0.10       | 0.30        | 0.18        |

同样，我们将周期设置为28，重复上面的训练过程，在$2017~2020$上的$Loss=0.33k$，在$2021$上的$Loss=0.46k$。

将周期设置为56，重复上面的训练过程，在$2017~2020$上的$Loss=0.32k$，在$2021$上的$Loss=0.46k$，可以发现周期变多的同时该$Model$在训练集上的$Loss$变小，但在数据集上的$Loss$没有变化，这就表明通过周期优化该模型已经到达了极限。

类似上述将feature乘weight再加bias得到y的Model被称为Linear Models。

但$Linear Model$仅能通过改变$weight$和$bias$进行线性表示，无法进行复杂关系的表示，这种限制被称为$Model Bias$。所以需要更复合的$Model$。

### Linear Model 优化

![image-20211026090819648](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026090819648.png)



如上图所示：我们发现红色折线无法用一个单一的线性函数去描述他，我们可以使用多个蓝色加和的结果去表示红色曲线。

**结论：**${\color{red} \texttt{red curve}} = constant + {\color{blue}\texttt{bulue curves}}$

![image-20211026165527451](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026165527451.png)



上面的一个例子中的${\color{red} \texttt{red curve}}$线是linear的，那我们可不可以表示出continuous curve呢？答案是可以的，只要我们的${\color{blue}\texttt{bulue curves}}$足够多就可以无限近似于曲线，如下图所示。

![image-20211026165543714](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026165543714.png)

如何表示出${\color{blue}\texttt{bulue curves}}$呢？上图中的折线被称为$\texttt{Hard Sigmoid}$，可以使用$\texttt{Sigmoid}$函数近似表示。在$\texttt{Sigmoid}$中通过改变${\color{red} c},{\color{green} b},{\color{blue} w}$的参数值可以表示出不同的${\color{blue}\texttt{bulue curves}}$。

${\color{red} c},{\color{green} b},{\color{blue} w}$对形状的影响如下图所示

![image-20211026170207791](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026170207791.png)



常数加带有不同参数的Sigmoid函数就可以正确拟合出各式各样的**分段**函数：

$$y=b+\sum\limits_{i}{\color{red} c_{i}}\ {\rm sigmoid}({\color{green} b_{i}}+{\color{blue} w_{i}}x_{1})$$

考虑多个feature的话可以在Model为$y=b+\sum\limits_{i}w_{j}x_{j}$的基础上按照上述方法可以改进为：

$$y=b+\sum\limits_{i}{\color{red} c_{i}}\ {\rm sigmoid}({\color{green} b_{i}}+\sum\limits_{j}{\color{blue} w_{ij}}x_{j})$$

**$Sigmoid$运算过程：**

$eg:\texttt{i:【1,2,3】 j:【1,2,3】}$计算 $$y=b+\sum\limits_{i}{\color{red} c_{i}}\ {\rm sigmoid}({\color{green} b_{i}}+\sum\limits_{j}{\color{blue} w_{ij}}x_{j})$$。

1.$sigmoid$函数内

$\texttt{+}$ 表示有三段分段函数，$x_{i}$表示features，$w_{ij}$表示第$i$个$Sigmoid$函数中$x_{j}feature$的$weight$（算出的是$sigmoid$括号里的值）

![image-20211026170801754](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026170801754.png)

使用矩阵、向量的运算方式可以简化其运算的表示过程。

$$r_{1}={\color{green} b_{1}}+{\color{blue} w_{11}}x_{1}+{\color{blue} w_{12}}x_{2}+{\color{blue} w_{13}}x_{3}\\ r_{2}={\color{green} b_{2}}+{\color{blue} w_{21}}x_{1}+{\color{blue} w_{22}}x_{2}+{\color{blue} w_{23}}x_{3}\\ r_{3}={\color{green} b_{3}}+{\color{blue} w_{31}}x_{1}+{\color{blue} w_{32}}x_{2}+{\color{blue} w_{33}}x_{3} $$

可以表示为矩阵/向量运算：

$$\begin{bmatrix} r_{1}\\ r_{2}\\ r_{3} \end{bmatrix} = \begin{bmatrix} {\color{green} b_{1}}\\ {\color{green} b_{2}}\\ {\color{green} b_{3}} \end{bmatrix} + \begin{bmatrix} {\color{blue} w_{11}} & {\color{blue} w_{12}} & {\color{blue} w_{13}}\\ {\color{blue} w_{21}} & {\color{blue} w_{22}} & {\color{blue} w_{23}}\\ {\color{blue} w_{31}} & {\color{blue} w_{32}} & {\color{blue} w_{33}} \end{bmatrix} \begin{bmatrix} x_{1}\\ x_{2}\\ x_{3} \end{bmatrix} $$

将矩阵/向量使用符号表示可以进一步简化为：

$$\pmb{r}=\pmb{b}+\pmb{W}\pmb{x}$$

2.Sigmoid函数的运算：

将$\texttt{r1,r2,r3}$的值通过$Sigmoid$得出结果$\texttt{a1,a2,a3}$

![image-20211026172731875](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026172731875.png)

使用矩阵/向量表示可以进一步简化为：$\pmb{a}=\pmb{\sigma}(\pmb{r})$

3.$Model$ 的计算

$Sigmoid$函数的结果还需要乘上$c_{i}$再求和，最后加上常数$b$：

![image-20211026172918283](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026172918283.png)

使用矩阵/向量表示可以进一步简化为：$y=b+\pmb{c}^{\mathsf{T}}\pmb{a}$

通过对上面三个步骤计算过程的整合可以将整个$Model$表示为：
$$
y=b+\pmb{c}^{\mathsf{T}}\pmb{\sigma}(\pmb{b}+\pmb{W}\pmb{x})
$$
该$Model$中所有的$hyperparameters$拼接起来形成向量$\pmb{\theta}$



### 问题简化

我们得到未知参数向量$\pmb{\theta}$后，我们目的仍是找到使$Model$中$Loss$最小的$\pmb{\theta}$，步骤如下：

1. 对未知参数$\pmb{\theta}$进行随机初始化，初始化结果为$\pmb{\theta}^{0}$。

2. 计算$\pmb{\theta}=\pmb{\theta}^{0}$时$\theta$对Loss的微分，将其计算结果组成的向量定义为$\pmb{g}$（即gradient）：
   $$
   \pmb{g}=
   \begin{bmatrix}
   \frac{\partial L}{\partial \theta_{1}}|_{\pmb{\theta}=\pmb{\theta}^{0}}\\
   \frac{\partial L}{\partial \theta_{2}}|_{\pmb{\theta}=\pmb{\theta}^{0}}\\
   \vdots
   \end{bmatrix}
   $$
   gradient也可以使用更简便的方式表达为：$\pmb{g}=\nabla L(\pmb{\theta}^{0})$

3. 反复update参数$\pmb{\theta}$的值：
   $$
   \begin{bmatrix}
   \theta_{1}^{1}\\
   \theta_{2}^{1}\\
   \vdots
   \end{bmatrix}
   \gets
    \begin{bmatrix}
   \theta_{1}^{0}\\
   \theta_{2}^{0}\\
   \vdots
   \end{bmatrix}
   -
   \begin{bmatrix}
   {\color{red} \eta}\frac{\partial L}{\partial \theta_{1}}|_{\pmb{\theta}=\pmb{\theta}^{0}}\\
   {\color{red} \eta}\frac{\partial L}{\partial \theta_{2}}|_{\pmb{\theta}=\pmb{\theta}^{0}}\\
   \vdots
   \end{bmatrix}
   $$
   其更新过程同样可以简化表达为：$\pmb{\theta}^{1}\gets\pmb{\theta^{0}}-{\color{red} \eta}\pmb{g}$

### 随机梯度下降

当训练集的数据量较大时，我们无法对所有数据都求出来$\pmb{g}$。所以我们可以把训练集拆分成$N$个$batch$，每次使用一个$batch$更新参数。每次更新的过程叫做$update$，当所有$batch$都更新一遍后称为一个$epoch$。

![image-20211026175023274](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026175023274.png)

### $ReLU$

$ReLU$函数：${\color{red} c}\max(0,{\color{green} b}+{\color{blue} w}x_{1})$如下图所示：两条蓝线的加和为$\texttt{Hard Sigmoid}$。

![image-20211026180322572](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026180322572.png)

### $Sigmoid$->$ReLU$

可以看出$ReLU$比$Sigmoid$函数更加贴合数据，因为它可以很容易表达出$\texttt{Hard Sigmoid}$。

由于一个$Sigmoid$要由两个折线函数合成（看上图），所以$ReLU$的参数会加倍

![image-20211026181601362](C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211026181601362.png)

$Sigmoid$函数和$ReLU$函数在机器学习中被称为$Activation Function$

### 优化方法总结

1. 增加$feature$

2. 增加$Activation Function$

3. 增加$Layer$(就是将上一个函式的输出作为下一个函式的输入)

4. 考虑训练级中的一些极端数据的影响

   

**当layer达到很多的数量时Machine Learning的过程被称为Deep Learning**

