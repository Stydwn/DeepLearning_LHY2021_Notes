<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>2Pytorch入门</title>
</head>
<body><h1 id='pytorch入门'>Pytorch入门</h1>
<h3 id='pytorchtensor介绍'>Pytorch/Tensor介绍</h3>
<p>利用Pytorch进行DNN的过程：</p>
<p><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027212655474.png" referrerpolicy="no-referrer" alt="image-20211027212655474"></p>
<p><strong>Tensor</strong>: 高纬度的矩阵</p>
<p><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027174956185.png" referrerpolicy="no-referrer" alt="image-20211027174956185"></p>
<h3 id='tensor基本操作'>tensor基本操作</h3>
<pre><code>dim in Pytorch == axis in Numpy
</code></pre>
<p><strong>初始化方法：</strong></p>
<pre><code class='language-python' lang='python'>#From list / NumPy array
x = torch.tensor([[1, -1], [-1, 1]])
x = torch.from_numpy(np.array([[1, -1], [-1, 1]]))
#Zero tensor
x = torch.zeros([2, 2])
#Unit tensor
x = torch.ones([1, 2, 5])
</code></pre>
<p><strong>Squeeze() and unsqueeze()：</strong></p>
<p>这两个方法分别对应<strong>减少一维</strong>和<strong>增加一维</strong>，注意只有维度为1时才会去掉或增添，里面的参数代表要增减维度的位置。</p>
<pre><code class='language-python' lang='python'>#input[1]:
import torch
import numpy as np
x = torch.zeros([1,2,1])
x = x.squeeze(-1)
print(&#39;squeeze(-1):&#39;,x.shape)
x = x.unsqueeze(-2)
print(&#39;unsqueeze(-2):&#39;,x.shape)
</code></pre>
<pre><code class='language-python' lang='python'>#output[1]:
squeeze(-1): torch.Size([1, 2])
unsqueeze(-2): torch.Size([1, 1, 2])
</code></pre>
<p><strong>transpose(dim0,dim1)</strong></p>
<p>转置操作，两个参数代表要转置的两维度。</p>
<pre><code class='language-python' lang='python'>#input[2]
a = torch.tensor([[[0,1,2],[3,4,5]]])
print(&quot;a.shape:&quot;, a.shape)
print(&#39;a:\n&#39;,a)
d = a.transpose(0,1)
print(&quot;d.shape:&quot;, d.shape)
print(&#39;d\n&#39;,d)
e = a.transpose(2,1)
print(&quot;e.shape:&quot;, e.shape)
print(&#39;e\n&#39;,e)
</code></pre>
<pre><code class='language-python' lang='python'>#output[2]
a.shape: torch.Size([1, 2, 3])
a:
 tensor([[[0, 1, 2],
         [3, 4, 5]]])
d.shape: torch.Size([2, 1, 3])
d
 tensor([[[0, 1, 2]],

        [[3, 4, 5]]])
e.shape: torch.Size([1, 3, 2])
e
 tensor([[[0, 3],
         [1, 4],
         [2, 5]]])
</code></pre>
<p><strong>cat()</strong></p>
<p>按指定的维度将张量拼接起来</p>
<pre><code class='language-python' lang='python'>#input[3]
x = torch.zeros(2,2,3)
y = torch.zeros(2,3,3)
z = torch.zeros(2,6,3)
w = torch.cat([x,y,z],1)
print(w.shape)
</code></pre>
<pre><code>#output[3]
torch.Size([2, 11, 3])
</code></pre>
<p><strong>+/-/pow()/sum()/mean()</strong></p>
<p>&nbsp;</p>
<h3 id='how-to-calculate-gradient'>How to Calculate Gradient?</h3>
<p><strong>eg:</strong><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027222916210.png" referrerpolicy="no-referrer" alt="image-20211027222916210"></p>
<pre><code>第一步：
x = torch.tensor([[1., 0.], [-1., 1.]], 
</code></pre>
<pre><code>第二步：
z = x.pow(2).sum()
</code></pre>
<pre><code>第三步：
z.backward()
</code></pre>
<pre><code class='language-python' lang='python'>第四步：
#input[4]
x.grad
#outpur[4]
tensor([[ 2.,  0.],
    	[-2.,  2.]])

</code></pre>
<p>&nbsp;</p>
<h3 id='读数据dataset--dataloader'>读数据Dataset &amp; Dataloader</h3>
<p>Dataset：对原始数据进行读取并进行预处理。</p>
<p>Dataloader：从Dataset中shuffle后的结果中拿取一部分data</p>
<p><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027223902298.png" referrerpolicy="no-referrer" alt="image-20211027223902298"></p>
<p>&nbsp;</p>
<h3 id='neural-network-layers全连接）'>Neural Network Layers（全连接）</h3>
<pre><code class='language-python' lang='python'>nn.Linear(in_features, out_features)
#eg:in_features = 32 out_features=64（输入的最后一维度必须是32）
</code></pre>
<p><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027224722185.png" referrerpolicy="no-referrer" alt="image-20211027224722185"></p>
<p>x：features W：联想到播放量的例子代表激活函数的个数xfeature的个数</p>
<p><img src="C:\Users\stydwn\AppData\Roaming\Typora\typora-user-images\image-20211027225343242.png" referrerpolicy="no-referrer" alt="image-20211027225343242"></p>
<h3 id='定义\nloss'>定义<mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="4.76ex" height="1.57ex" role="img" focusable="false" viewBox="0 -683 2104 694" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.025ex;"><defs><path id="MJX-3-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-3-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43F" xlink:href="#MJX-3-TEX-I-1D43F"></use></g><g data-mml-node="mi" transform="translate(681,0)"><use data-c="1D45C" xlink:href="#MJX-3-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(1166,0)"><use data-c="1D460" xlink:href="#MJX-3-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(1635,0)"><use data-c="1D460" xlink:href="#MJX-3-TEX-I-1D460"></use></g></g></g></svg></mjx-container><script type="math/tex">Loss</script></h3>
<p>对于回归问题：</p>
<pre><code>MSELoss()
</code></pre>
<p>对于分类问题：</p>
<pre><code>CrossEntropyLoss()
</code></pre>
<p>&nbsp;</p>
<h3 id='定义model'>定义Model：</h3>
<pre><code class='language-python' lang='python'>import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.net = nn.Sequential(
        	
            nn.Linear(10, 32)  #第一层laye
            
            nn.Sigmoid()       #激活函数
            
            nn.Linear(32, 1)   #第二层layer
        )

    def forward(self, x):
        return self.net(x)     #返回你定义的Model
</code></pre>
<p>上述代码过程就是在激活函数是<mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="8.36ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 3695 910" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.464ex;"><defs><path id="MJX-4-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-4-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-4-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-4-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D446" xlink:href="#MJX-4-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(645,0)"><use data-c="1D456" xlink:href="#MJX-4-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(990,0)"><use data-c="1D454" xlink:href="#MJX-4-TEX-I-1D454"></use></g><g data-mml-node="mi" transform="translate(1467,0)"><use data-c="1D45A" xlink:href="#MJX-4-TEX-I-1D45A"></use></g><g data-mml-node="mi" transform="translate(2345,0)"><use data-c="1D45C" xlink:href="#MJX-4-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(2830,0)"><use data-c="1D456" xlink:href="#MJX-4-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(3175,0)"><use data-c="1D451" xlink:href="#MJX-4-TEX-I-1D451"></use></g></g></g></svg></mjx-container><script type="math/tex">Sigmoid</script>的条件下先通过第一层layer输出32features在通过一层 layer输出1features。</p>
<p>&nbsp;</p>
<h3 id='优化步骤optimization-algorithms-'>优化步骤<strong>Optimization algorithms</strong> ：</h3>
<pre><code class='language-python' lang='python'>torch.optim.SGD(model.parameters(),learning rate, momentum = 0)
</code></pre>
<p>&nbsp;</p>
<h3 id='训练步骤'>训练步骤：</h3>
<pre><code class='language-python' lang='python'>dataset = MyDataset(file)  #读数据并初始化
tr_set = DataLoader(dataset, 16, shuffle=True) #取出datase中的部分数据作为训练集
model = MyModel().to(device)  #modle的训练机器
criterion = nn.MSELoss() #损失函数
optimizer = torch.optim.SGD(model.parameters(), 0.1) #优化方法

</code></pre>
<pre><code class='language-python' lang='python'>for epoch in range(n_epochs): # 训练的epoch
    	model.train() # 设置model traning状态
    	for x, y in tr_set: # x代表feature y代表label
        	optimizer.zero_grad() # 将gradient设为0，防止上一步未清空的梯度
        	x, y = x.to(device), y.to(device) #移动data到训练机器中
        	pred = model(x) # 该Model输出的结果
        	loss = criterion(pred, y) # 算出该Model的Loss
        	loss.backward() # 算出gradient
        	optimizer.step()# 依据上一步算出的gradient更新Model 
</code></pre>
<p>验证模型在训练集性能：</p>
<pre><code class='language-python' lang='python'>model.eval() # 设置模型为evaluation mode
total_loss = 0 # 初始化loss
for x, y in dv_set:
    	x, y = x.to(device), y.to(device)
    	with torch.no_grad():# 设置Model不算gradient
        	pred = model(x) # 该Model输出的结果
        	loss = criterion(pred, y)
    	total_loss += loss.cpu().item() * len(x) # 累加loss
	avg_loss = total_loss / len(dv_set.dataset) # 取均值

</code></pre>
<p>输入预测结果：</p>
<pre><code>model.eval() # 设置模型为evaluation mode
preds = []
for x in tt_set:
    x = x.to(device)
    with torch.no_grad(): # 设置Model不算gradient
        pred = model(x) # 预测结果
        preds.append(pred.cpu()) # 收集预测的结果

</code></pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>